title: Описание

Модуль TestUtility содержит набор функций для создания тестовых сценариев по внутреннему тестированию различных модулей.

Набор функций модуля:
beginTest         - инициализирует новый тест
endTest           - завершает тест с указанием результата (OK, FAILED). В случае
                    неуспешного выполнения теста к результату добавляется
                    сообщение с описанием причины неудачи
failTest          - завершение теста при неуспешном результате
addTestInfo       - добавляет дополнительную информацию по тесту
getTestTimeSecond - возвращает время выполнения теста в секундах
compareChar       - сравнивает текущую строку с ожидаемой и в случае расхождений
                    завершает тест с неуспешным результатом
compareRowCount   - сравнивает текущее кол-во строк в источнике (таблица, ref-курсор)
                    с ожидаемым и в случае расхождений завершает тест с неуспешным
                    результатом

Структура теста:

В общем простейшем случае структура теста выглядит следующим образом
> начало теста
>   результат := проверяемая_функция();
>   if результат != ожидаемый_результат then
>     завершаем тест с неуспешным результатом;
>   end if;
> завершение теста

Пример:

(code)

...
pkg_TestUtility.beginTest( ... );
begin
  testId := pkg_<<PACKAGE_NAME>>.create( ... );

  if testId is not null then
    if not isException then
      pkg_TestUtility.compareRowCount(
          tableName        => 'test_table'
        , filterCondition  => 'test_id = ' || to_char( testId )
        , expectedRowCount => 1
        , failMessageText  => 'Test row is not created'
        );
    else
      pkg_TestUtility.failTest( 'Must be exception as result' );
    end if;
  else
    pkg_TestUtility.failTest( 'Test row Id is null' );
 end if;

exception
  when others then
    if isException then
      logger.trace( 'Message: ' || pkg_Logging.getErrorStack() );
    else
      pkg_TestUtility.failTest( 'Exception: ' || pkg_Logging.getErrorStack() );
    end if;
end;
pkg_TestUtility.endTest();
...

(end)

В данном тесте по переданному набору параметров создается тестовая строка (с помощью
вызова pkg_<<PACKAGE_NAME>>.create()) и анализируется результат её создания.

В зависимости от тестового сценария, корректным результатом может быть один из следующих вариантов

* Запись создана, её идентификатор передан в testId и в таблице
  test_table есть запись с test_id = to_char( testId )

* Функция создания записи pkg_<<PACKAGE_NAME>>.create() завершилась с ошибкой (вернула исключение)

* Функция создания записи отработала без ошибок, но идентификатор testId пустой

* Функция создания записи отработала без ошибок, но по возвращенному
  идентификатору testId нет записей в таблице test_table

* и т.п.

Можно составить несколько тестовых сценариев, в которых будут проверяться ситуации
создания записей с отсутствием каких-нибудь обязательных параметров, с
недопустимым сочетанием параметров и т.п.

Аналогичным образом можно создать следующие тестовые сценарии

* обновление параметров записей - создается тестовая запись и обновляется один
  (или несколько) её параметров. Далее с помощью
  pkg_TestUtility.compareRowCount() проверяется, что эти параметры действительно
  изменили своё значение. Также может анализироваться ситуация, при которой
  корректным поведением будет возврат исключения (например, при указании
  недопустимого значения обновляемого параметра)

* удаление записей - создается тестовая строка (или используется
  существующая) и удаляется с помощью интерфейсной функции. Корректным результатом
  может быть установка признака deleted = 1 (при логическом удалении), либо
  отсутствие записи в таблице (при физическом удалении)

* поиск записей (find) - создается тестовая строка (или
  используется существующая) и с помощью интерфейсной функции выполняется её
  поиск. Возвращаемый ref-курсор с помощью pkg_TestUtility.compareRowCount()
  анализируется на наличие требуемой записи

* получение списка строк (get) - создается тестовая запись
  (или используется существующая) и с помощью интерфейсной функции выполняется
  получение списка, в котором она должна быть. Возвращаемый ref-курсор со
  списком с помощью pkg_TestUtility.compareRowCount() анализируется на наличие
  требуемой записи

Пример тестирования длительности работы функции/процедуры:

(code)

begin
  ...
  pkg_TestUtility.beginTest( ... );

  rc := pkg_<<PACKAGE_NAME>>.find( ... );

  responseTime := pkg_TestUtility.getTestTimeSecond();
  pkg_TestUtility.addTestInfo( ' (' || to_char( responseTime ) || ' sec)' );

  if responseTime > timeLimitSec then
    pkg_TestUtility.failTest( 'Response time of find() exceeds ' || to_char( timeLimitSec ) || ' seconds!' );
  end if;
  pkg_TestUtility.endTest();
  ...
end;

(end)

pkg_<<PACKAGE_NAME>>.find() - тестируемая функция

В данном примере используется функция получения длительности выполнения теста
pkg_TestUtility.getTestTimeSecond(). Отсчет времени начинается от момента вызова
pkg_TestUtility.beginTest(). Полученное время работы теста (responseTime) можно
сравнить с эталонным временем (timeLimitSec в примере) и прервать тест, если
производительность тестируемой функции неприемлема.

Следует помнить, что повторный вызов функции при незначительном изменении
параметров обычно выполняется значительно быстрее, что объясняется загрузкой
используемых блоков индексов/данных в буфер-кэш БД. Принимая это во внимание,
рекомендуется создавать не менее двух тестов на производительность одной функции,
первый тест из которых имеет больший лимит по времени, чем второй и последующие.
Другими словами, например, пользователи могут подождать 10-20 секунд при первом
вызове функции, если последующие запуски не будут превышать 1 секунду, что и
можно отразить в виде сценариев.

Структура в OMS:

Функционал по внутреннему тестированию модуля размещается в целевом модуле по
пути DB/Test и включает в себя пакет с именем pkg_<Имя модуля>Test, набор
скриптов *.sql с реализацией тестовых сценариев и скрипт run.sql для выполнения
тестовых сценариев в нужном порядке.

Установка пакета для внутреннего тестирования модуля выполняется с помощью команды:

(code)

$ make install-test LOAD_USERID=???/??? LOAD_OPERATORID=???/???

(end)

Запуск внутреннего тестирования выполняется с помощью команды (для OMS версии 1.6.4 и выше):

(code)

$ make test LOAD_USERID=???/??? LOAD_OPERATORID=???/???

(end)

Пример вывода:

> $ make install-test LOAD_USERID=???/???@??? LOAD_OPERATORID=???/???
> Test/pkg_Test.pks: -> ???@??? ...
> Package created.
> No errors.
> Test/pkg_Test.pkb: -> ???@??? ...
> Package body created.
> No errors.
>
> $ make test LOAD_USERID=scoring/???@??? LOAD_OPERATORID=???/???
> Test/run.sql: -> ???@??? ...
> Test/test-row.sql: ...
> 16:09:41,542:      : INFO : Create test row (normal)                                : OK
> 16:09:41,557:    15: INFO : Create test row (invalid code)                          : OK
> 16:09:41,588:    15: INFO : Update test row                                         : OK
> 16:09:41,620:    16: INFO : Delete test row (normal)                                : OK
> 16:09:41,620:     0: INFO : Delete test row (without test_id)                       : OK
> 16:09:41,651:    31: INFO : Find test row (id)                                      : OK
> 16:09:41,682:    15: INFO : Get test row list                                       : OK
> PL/SQL procedure successfully completed.
> No errors.

По умолчанию, при запуске команды, приведенной выше, осуществляется выполнение
скрипта run.sql из каталога DB/Test. Если необходимо выполнить другой скрипт, то
в команду нужно добавить переменную TEST_SCRIPT с указанием имени скрипта,
например TEST_SCRIPT=logic.sql

Для тестирования отсутствует версионность (указание версии в INSTALL_VERSION),
т.к. при любой доработке модуля должны успешно выполняться ВСЕ внутренние тесты.
Если какие-то из тестов потеряли свою актуальность, то их нужно обновить / удалить.

Заключительные замечания:

Хорошей практикой является создание тестовых сценариев по очередной доработке ДО
начала этой доработки (т.н. TDD (Test Driven Development)). Т.е. до начала
доработки придумывается и реализуется набор сценариев, которыми можно будет
проверить, что доработка выполнена корректно (что функционал работает в
соответствии с требованиями к нему). Естественно, что все тестовые сценарии
вначале будут завершены неуспешно, т.к. на тот момент ещё отсутствует проверяемый
функционал, однако, по мере выполнения доработки всё больше и больше тестов будут
завершаться успешно...

При поступлении инцидентов от тестировщиков / пользователей необходимо сначала
реализовать тестовый сценарий, который повторяет ошибочную ситуацию и только после
этого приступать к исправлению, в итоге добившись того, что тестовый сценарий
завершится успешно.

_Естественно, что такой подход в начале приведет к некоторому увеличению времени
на разработку, однако, потом всё это многократно окупится, благодаря снижению
временных затрат на тестирование (т.к. один раз качественно написанные тесты могут
служить гарантией корректного функционирования модуля) и значительному снижению
кол-ва ошибок, на исправление которых нужно будет потом отвлекаться от других задач._